{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fcb4cfd",
   "metadata": {
    "origin_pos": 1
   },
   "source": [
    "# Concise Implementation of Linear Regression\n",
    ":label:`sec_linear_concise`\n",
    "\n",
    "Deep learning has witnessed a sort of Cambrian explosion\n",
    "over the past decade.\n",
    "The sheer number of techniques, applications and algorithms by far surpasses the\n",
    "progress of previous decades. \n",
    "This is due to a fortuitous combination of multiple factors,\n",
    "one of which is the powerful free tools\n",
    "offered by a number of open-source deep learning frameworks.\n",
    "Theano :cite:`Bergstra.Breuleux.Bastien.ea.2010`,\n",
    "DistBelief :cite:`Dean.Corrado.Monga.ea.2012`,\n",
    "and Caffe :cite:`Jia.Shelhamer.Donahue.ea.2014`\n",
    "arguably represent the\n",
    "first generation of such models \n",
    "that found widespread adoption.\n",
    "In contrast to earlier (seminal) works like\n",
    "SN2 (Simulateur Neuristique) :cite:`Bottou.Le-Cun.1988`,\n",
    "which provided a Lisp-like programming experience,\n",
    "modern frameworks offer automatic differentiation\n",
    "and the convenience of Python.\n",
    "These frameworks allow us to automate and modularize\n",
    "the repetitive work of implementing gradient-based learning algorithms.\n",
    "\n",
    "In :numref:`sec_linear_scratch`, we relied only on\n",
    "(i) tensors for data storage and linear algebra;\n",
    "and (ii) automatic differentiation for calculating gradients.\n",
    "In practice, because data iterators, loss functions, optimizers,\n",
    "and neural network layers\n",
    "are so common, modern libraries implement these components for us as well.\n",
    "In this section, (**we will show you how to implement\n",
    "the linear regression model**) from :numref:`sec_linear_scratch`\n",
    "(**concisely by using high-level APIs**) of deep learning frameworks.\n",
    "\n",
    "# 선형 회귀의 간결한 구현\n",
    ":label:`sec_linear_concise`\n",
    "\n",
    "딥 러닝은 지난 10년 동안 캄브리아기 폭발과 같은 급격한 발전을 경험했습니다.엄청난 양의 기술, 응용 프로그램, 알고리즘이 개발되면서 지난 수십 년간의 발전을 훨씬 뛰어넘었습니다.이는 여러 요인의 우연한 조합 덕분인데,\n",
    "그중 하나는 여러 오픈소스 딥 러닝 프레임워크에서 제공하는 강력한 무료 도구입니다.Theano(:cite:`Bergstra.Breuleux.Bastien.ea.2010`),\n",
    "DistBelief(:cite:`Dean.Corrado.Monga.ea.2012`),\n",
    "Caffe(:cite:`Jia.Shelhamer.Donahue.ea.2014`)는 널리 채택된 이러한 모델의 첫 번째 세대를 대표한다고 할 수 있습니다.Lisp와 유사한 프로그래밍 경험을 제공했던\n",
    "SN2(신경 시뮬레이터) :cite:`Bottou.Le-Cun.1988`와 같은 초기(획기적인) 작업과는 달리,\n",
    "최신 프레임워크는 자동 미분과\n",
    "Python의 편의성을 제공합니다.이러한 프레임워크를 통해 기울기 기반 학습 알고리즘 구현의 반복적인 작업을 자동화하고 모듈화할 수 있습니다.:numref:`sec_linear_scratch`에서는\n",
    "(i) 데이터 저장 및 선형 대수를 위한 텐서,\n",
    "(ii) 기울기 계산을 위한 자동 미분에만 의존했습니다.실제로 데이터 반복자, 손실 함수, 옵티마이저,\n",
    "신경망 계층은 매우 일반적이기 때문에\n",
    "최신 라이브러리는 이러한 구성 요소도 자동으로 구현합니다.이 섹션에서는 (**:numref:`sec_linear_scratch`를 사용하여\n",
    "선형 회귀 모델을 구현하는 방법을 보여드리겠습니다.**)\n",
    "(**고수준 API를 사용하여 간략하게**) 딥러닝 프레임워크를 다룹니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "977e1a26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:07:11.783410Z",
     "iopub.status.busy": "2023-08-18T07:07:11.782979Z",
     "iopub.status.idle": "2023-08-18T07:07:14.758863Z",
     "shell.execute_reply": "2023-08-18T07:07:14.757904Z"
    },
    "origin_pos": 3,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02924a0c",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "## Defining the Model\n",
    "\n",
    "When we implemented linear regression from scratch\n",
    "in :numref:`sec_linear_scratch`,\n",
    "we defined our model parameters explicitly\n",
    "and coded up the calculations to produce output\n",
    "using basic linear algebra operations.\n",
    "You *should* know how to do this.\n",
    "But once your models get more complex,\n",
    "and once you have to do this nearly every day,\n",
    "you will be glad of the assistance.\n",
    "The situation is similar to coding up your own blog from scratch.\n",
    "Doing it once or twice is rewarding and instructive,\n",
    "but you would be a lousy web developer\n",
    "if you spent a month reinventing the wheel.\n",
    "\n",
    "For standard operations,\n",
    "we can [**use a framework's predefined layers,**]\n",
    "which allow us to focus\n",
    "on the layers used to construct the model\n",
    "rather than worrying about their implementation.\n",
    "Recall the architecture of a single-layer network\n",
    "as described in :numref:`fig_single_neuron`.\n",
    "The layer is called *fully connected*,\n",
    "since each of its inputs is connected\n",
    "to each of its outputs\n",
    "by means of a matrix--vector multiplication.\n",
    "\n",
    "## 모델 정의\n",
    "\n",
    ":numref:`sec_linear_scratch`에서 선형 회귀를 처음부터 구현할 때,\n",
    "모델 매개변수를 명시적으로 정의하고\n",
    "기본적인 선형 대수 연산을 사용하여 출력을 생성하는 계산을 코딩했습니다.이 작업을 수행하는 방법은 이미 알고 계실 겁니다.하지만 모델이 더 복잡해지고\n",
    "거의 매일 이 작업을 수행해야 한다면\n",
    "도움이 될 것입니다.이 상황은 블로그를 처음부터 코딩하는 것과 비슷합니다.한두 번 정도 해 보면 보람 있고 유익하지만\n",
    "한 달 동안 바퀴를 새로 만드는 데 시간을 허비한다면\n",
    "형편없는 웹 개발자가 될 것입니다.표준 연산의 경우\n",
    "[**프레임워크의 미리 정의된 레이어를 사용**]할 수 있습니다. 이를 통해\n",
    "모델을 구성하는 데 사용된 레이어에 집중할 수 있으며\n",
    "구현에 신경 쓸 필요가 없습니다.:numref:`fig_single_neuron`에서 설명한 단일 레이어 네트워크의 아키텍처를 생각해 보세요.이 계층은 *완전 연결* 계층이라고 합니다.\n",
    "각 입력이 행렬-벡터 곱셈을 통해\n",
    "각 출력에 연결되어 있기 때문입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c272de",
   "metadata": {
    "origin_pos": 8,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "In PyTorch, the fully connected layer is defined in `Linear` and `LazyLinear` classes (available since version 1.8.0). \n",
    "The latter\n",
    "allows users to specify *merely*\n",
    "the output dimension,\n",
    "while the former\n",
    "additionally asks for\n",
    "how many inputs go into this layer.\n",
    "Specifying input shapes is inconvenient and may require nontrivial calculations\n",
    "(such as in convolutional layers).\n",
    "Thus, for simplicity, we will use such \"lazy\" layers\n",
    "whenever we can.\n",
    "\n",
    "PyTorch에서 완전 연결 계층은 `Linear` 및 `LazyLinear` 클래스(1.8.0 버전부터 사용 가능)에 정의되어 있습니다.후자는 사용자가 *단지* 출력 차원만 지정할 수 있도록 하는 반면, `LazyLinear`는 이 계층에 몇 개의 입력을 입력할지 추가로 묻습니다.입력 형태를 지정하는 것은 불편하며, (합성곱 계층과 같은)\n",
    "간단하지 않은 계산이 필요할 수 있습니다.따라서 단순화를 위해 가능한 한 이러한 \"지연\" 계층을 사용할 것입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce018c39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:07:14.762918Z",
     "iopub.status.busy": "2023-08-18T07:07:14.762514Z",
     "iopub.status.idle": "2023-08-18T07:07:14.768089Z",
     "shell.execute_reply": "2023-08-18T07:07:14.767293Z"
    },
    "origin_pos": 10,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class LinearRegression(d2l.Module):  #@save\n",
    "    \"\"\"The linear regression model implemented with high-level APIs.\"\"\"\n",
    "    def __init__(self, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.LazyLinear(1)\n",
    "        self.net.weight.data.normal_(0, 0.01)\n",
    "        self.net.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6f6937",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "In the `forward` method we just invoke the built-in `__call__` method of the predefined layers to compute the outputs.\n",
    "\n",
    "`forward` 메서드에서는 미리 정의된 레이어의 내장 `__call__` 메서드를 호출하여 출력을 계산합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0cd2d1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:07:14.771292Z",
     "iopub.status.busy": "2023-08-18T07:07:14.770994Z",
     "iopub.status.idle": "2023-08-18T07:07:14.775035Z",
     "shell.execute_reply": "2023-08-18T07:07:14.774274Z"
    },
    "origin_pos": 13,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "@d2l.add_to_class(LinearRegression)  #@save\n",
    "def forward(self, X):\n",
    "    return self.net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc711b8c",
   "metadata": {
    "origin_pos": 15
   },
   "source": [
    "## Defining the Loss Function\n",
    "\n",
    "## 손실 함수 정의\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275f4aec",
   "metadata": {
    "origin_pos": 17,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[**The `MSELoss` class computes the mean squared error (without the $1/2$ factor in :eqref:`eq_mse`).**]\n",
    "By default, `MSELoss` returns the average loss over examples.\n",
    "It is faster (and easier to use) than implementing our own.\n",
    "\n",
    "[**`MSELoss` 클래스는 평균 제곱 오차( :eqref:`eq_mse`의 $1/2$ 요소 제외)를 계산합니다.**]\n",
    "기본적으로 `MSELoss`는 예제에 대한 평균 손실을 반환합니다.이 방법은 직접 구현하는 것보다 더 빠르고 사용하기도 쉽습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "364e263d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:07:14.778397Z",
     "iopub.status.busy": "2023-08-18T07:07:14.777850Z",
     "iopub.status.idle": "2023-08-18T07:07:14.781898Z",
     "shell.execute_reply": "2023-08-18T07:07:14.781081Z"
    },
    "origin_pos": 19,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "@d2l.add_to_class(LinearRegression)  #@save\n",
    "def loss(self, y_hat, y):\n",
    "    fn = nn.MSELoss()\n",
    "    return fn(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401fd6a5",
   "metadata": {
    "origin_pos": 21
   },
   "source": [
    "## Defining the Optimization Algorithm\n",
    "\n",
    "## 최적화 알고리즘 정의\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed9ee40",
   "metadata": {
    "origin_pos": 23,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "Minibatch SGD is a standard tool\n",
    "for optimizing neural networks\n",
    "and thus PyTorch supports it alongside a number of\n",
    "variations on this algorithm in the `optim` module.\n",
    "When we (**instantiate an `SGD` instance,**)\n",
    "we specify the parameters to optimize over,\n",
    "obtainable from our model via `self.parameters()`,\n",
    "and the learning rate (`self.lr`)\n",
    "required by our optimization algorithm.\n",
    "\n",
    "미니배치 SGD는 신경망 최적화를 위한 표준 도구이며, PyTorch는 `optim` 모듈에서 이 알고리즘의 다양한 변형과 함께 미니배치 SGD를 지원합니다.(**`SGD` 인스턴스를 인스턴스화**할 때) `self.parameters()`를 통해 모델에서 얻을 수 있는 최적화 대상 매개변수와 최적화 알고리즘에 필요한 학습률(`self.lr`)을 지정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89f6157c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:07:14.785329Z",
     "iopub.status.busy": "2023-08-18T07:07:14.784736Z",
     "iopub.status.idle": "2023-08-18T07:07:14.788791Z",
     "shell.execute_reply": "2023-08-18T07:07:14.788006Z"
    },
    "origin_pos": 25,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "@d2l.add_to_class(LinearRegression)  #@save\n",
    "def configure_optimizers(self):\n",
    "    return torch.optim.SGD(self.parameters(), self.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df621aa",
   "metadata": {
    "origin_pos": 26
   },
   "source": [
    "## Training\n",
    "\n",
    "You might have noticed that expressing our model through\n",
    "high-level APIs of a deep learning framework\n",
    "requires fewer lines of code.\n",
    "We did not have to allocate parameters individually,\n",
    "define our loss function, or implement minibatch SGD.\n",
    "Once we start working with much more complex models,\n",
    "the advantages of the high-level API will grow considerably.\n",
    "\n",
    "Now that we have all the basic pieces in place,\n",
    "[**the training loop itself is the same\n",
    "as the one we implemented from scratch.**]\n",
    "So we just call the `fit` method (introduced in :numref:`oo-design-training`),\n",
    "which relies on the implementation of the `fit_epoch` method\n",
    "in :numref:`sec_linear_scratch`,\n",
    "to train our model.\n",
    "\n",
    "## 학습\n",
    "\n",
    "딥 러닝 프레임워크의 고수준 API를 통해 모델을 표현하는 것이\n",
    "코드 줄 수를 줄인다는 것을 눈치채셨을 것입니다.매개변수를 개별적으로 할당하거나,\n",
    "손실 함수를 정의하거나, 미니배치 SGD를 구현할 필요가 없었습니다.훨씬 더 복잡한 모델을 다루기 시작하면\n",
    "고수준 API의 이점이 상당히 커질 것입니다.이제 모든 기본 요소가 준비되었으므로\n",
    "[**학습 루프 자체는\n",
    "처음부터 구현한 루프와 동일합니다.**]\n",
    "따라서 :numref:`oo-design-training`에서 소개된 `fit` 메서드를 호출하기만 하면 됩니다.\n",
    "이 메서드는 :numref:`sec_linear_scratch`에서 구현된 `fit_epoch` 메서드를 사용하여\n",
    "모델을 학습시킵니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "213db678",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:07:14.792149Z",
     "iopub.status.busy": "2023-08-18T07:07:14.791601Z",
     "iopub.status.idle": "2023-08-18T07:07:16.625923Z",
     "shell.execute_reply": "2023-08-18T07:07:16.624704Z"
    },
    "origin_pos": 27,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"237.376562pt\" height=\"183.35625pt\" viewBox=\"0 0 237.376562 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-07-11T17:24:46.663362</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.7.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 183.35625 \n",
       "L 237.376562 183.35625 \n",
       "L 237.376562 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 26.925 145.8 \n",
       "L 222.225 145.8 \n",
       "L 222.225 7.2 \n",
       "L 26.925 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path id=\"m1e9499e3da\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1e9499e3da\" x=\"26.925\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(18.973438 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1e9499e3da\" x=\"59.475\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 0.5 -->\n",
       "      <g transform=\"translate(51.523438 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1e9499e3da\" x=\"92.025\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 1.0 -->\n",
       "      <g transform=\"translate(84.073438 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1e9499e3da\" x=\"124.575\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 1.5 -->\n",
       "      <g transform=\"translate(116.623437 160.398438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1e9499e3da\" x=\"157.125\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 2.0 -->\n",
       "      <g transform=\"translate(149.173438 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1e9499e3da\" x=\"189.675\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 2.5 -->\n",
       "      <g transform=\"translate(181.723438 160.398438) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_7\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m1e9499e3da\" x=\"222.225\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 3.0 -->\n",
       "      <g transform=\"translate(214.273438 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"95.410156\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_8\">\n",
       "     <!-- epoch -->\n",
       "     <g transform=\"translate(109.346875 174.076563) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <defs>\n",
       "       <path id=\"m744c99d54e\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m744c99d54e\" x=\"26.925\" y=\"139.503507\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(13.5625 143.302726) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m744c99d54e\" x=\"26.925\" y=\"100.060782\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 5 -->\n",
       "      <g transform=\"translate(13.5625 103.860001) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m744c99d54e\" x=\"26.925\" y=\"60.618057\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 10 -->\n",
       "      <g transform=\"translate(7.2 64.417276) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m744c99d54e\" x=\"26.925\" y=\"21.175332\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 15 -->\n",
       "      <g transform=\"translate(7.2 24.974551) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_12\">\n",
       "    <path d=\"M 42.182813 13.5 \n",
       "\" clip-path=\"url(#p5e0b4692ab)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_13\">\n",
       "    <path d=\"M 42.182813 13.5 \n",
       "L 74.732812 119.135527 \n",
       "\" clip-path=\"url(#p5e0b4692ab)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_14\">\n",
       "    <path d=\"M 42.182813 13.5 \n",
       "L 74.732812 119.135527 \n",
       "\" clip-path=\"url(#p5e0b4692ab)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_15\">\n",
       "    <path d=\"M 92.025 133.56649 \n",
       "\" clip-path=\"url(#p5e0b4692ab)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_16\">\n",
       "    <path d=\"M 42.182813 13.5 \n",
       "L 74.732812 119.135527 \n",
       "L 107.282813 136.53357 \n",
       "\" clip-path=\"url(#p5e0b4692ab)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_17\">\n",
       "    <path d=\"M 92.025 133.56649 \n",
       "\" clip-path=\"url(#p5e0b4692ab)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_18\">\n",
       "    <path d=\"M 42.182813 13.5 \n",
       "L 74.732812 119.135527 \n",
       "L 107.282813 136.53357 \n",
       "L 139.832812 139.169719 \n",
       "\" clip-path=\"url(#p5e0b4692ab)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_19\">\n",
       "    <path d=\"M 92.025 133.56649 \n",
       "\" clip-path=\"url(#p5e0b4692ab)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_20\">\n",
       "    <path d=\"M 42.182813 13.5 \n",
       "L 74.732812 119.135527 \n",
       "L 107.282813 136.53357 \n",
       "L 139.832812 139.169719 \n",
       "\" clip-path=\"url(#p5e0b4692ab)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_21\">\n",
       "    <path d=\"M 92.025 133.56649 \n",
       "L 157.125 139.37302 \n",
       "\" clip-path=\"url(#p5e0b4692ab)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_22\">\n",
       "    <path d=\"M 42.182813 13.5 \n",
       "L 74.732812 119.135527 \n",
       "L 107.282813 136.53357 \n",
       "L 139.832812 139.169719 \n",
       "L 172.382812 139.439505 \n",
       "\" clip-path=\"url(#p5e0b4692ab)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_23\">\n",
       "    <path d=\"M 92.025 133.56649 \n",
       "L 157.125 139.37302 \n",
       "\" clip-path=\"url(#p5e0b4692ab)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_24\">\n",
       "    <path d=\"M 42.182813 13.5 \n",
       "L 74.732812 119.135527 \n",
       "L 107.282813 136.53357 \n",
       "L 139.832812 139.169719 \n",
       "L 172.382812 139.439505 \n",
       "L 204.932812 139.493104 \n",
       "\" clip-path=\"url(#p5e0b4692ab)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_25\">\n",
       "    <path d=\"M 92.025 133.56649 \n",
       "L 157.125 139.37302 \n",
       "\" clip-path=\"url(#p5e0b4692ab)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_26\">\n",
       "    <path d=\"M 42.182813 13.5 \n",
       "L 74.732812 119.135527 \n",
       "L 107.282813 136.53357 \n",
       "L 139.832812 139.169719 \n",
       "L 172.382812 139.439505 \n",
       "L 204.932812 139.493104 \n",
       "\" clip-path=\"url(#p5e0b4692ab)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"line2d_27\">\n",
       "    <path d=\"M 92.025 133.56649 \n",
       "L 157.125 139.37302 \n",
       "L 222.225 139.5 \n",
       "\" clip-path=\"url(#p5e0b4692ab)\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 26.925 145.8 \n",
       "L 26.925 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 222.225 145.8 \n",
       "L 222.225 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 26.925 145.8 \n",
       "L 222.225 145.8 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 26.925 7.2 \n",
       "L 222.225 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 135.634375 45.1125 \n",
       "L 215.225 45.1125 \n",
       "Q 217.225 45.1125 217.225 43.1125 \n",
       "L 217.225 14.2 \n",
       "Q 217.225 12.2 215.225 12.2 \n",
       "L 135.634375 12.2 \n",
       "Q 133.634375 12.2 133.634375 14.2 \n",
       "L 133.634375 43.1125 \n",
       "Q 133.634375 45.1125 135.634375 45.1125 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_28\">\n",
       "     <path d=\"M 137.634375 20.298438 \n",
       "L 147.634375 20.298438 \n",
       "L 157.634375 20.298438 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_13\">\n",
       "     <!-- train_loss -->\n",
       "     <g transform=\"translate(165.634375 23.798438) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-5f\" d=\"M 3263 -1063 \n",
       "L 3263 -1509 \n",
       "L -63 -1509 \n",
       "L -63 -1063 \n",
       "L 3263 -1063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"232.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"282.763672\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"310.546875\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"371.728516\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"423.828125\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"line2d_29\">\n",
       "     <path d=\"M 137.634375 35.254688 \n",
       "L 147.634375 35.254688 \n",
       "L 157.634375 35.254688 \n",
       "\" style=\"fill: none; stroke-dasharray: 5.55,2.4; stroke-dashoffset: 0; stroke: #ff7f0e; stroke-width: 1.5\"/>\n",
       "    </g>\n",
       "    <g id=\"text_14\">\n",
       "     <!-- val_loss -->\n",
       "     <g transform=\"translate(165.634375 38.754688) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-76\" d=\"M 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 563 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2284 0 \n",
       "L 1503 0 \n",
       "L 191 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-76\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"59.179688\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"120.458984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-5f\" x=\"148.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"198.242188\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"226.025391\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"287.207031\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-73\" x=\"339.306641\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p5e0b4692ab\">\n",
       "   <rect x=\"26.925\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LinearRegression(lr=0.03)\n",
    "data = d2l.SyntheticRegressionData(w=torch.tensor([2, -3.4]), b=4.2)\n",
    "trainer = d2l.Trainer(max_epochs=3)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565d622c",
   "metadata": {
    "origin_pos": 28
   },
   "source": [
    "Below, we\n",
    "[**compare the model parameters learned\n",
    "by training on finite data\n",
    "and the actual parameters**]\n",
    "that generated our dataset.\n",
    "To access parameters,\n",
    "we access the weights and bias\n",
    "of the layer that we need.\n",
    "As in our implementation from scratch,\n",
    "note that our estimated parameters\n",
    "are close to their true counterparts.\n",
    "\n",
    "아래에서는\n",
    "[**유한한 데이터로 학습하여 학습한 모델 매개변수와\n",
    "데이터셋을 생성한 실제 매개변수를 비교합니다.**]\n",
    "매개변수에 접근하기 위해\n",
    "필요한 계층의 가중치와 편향에 접근합니다.처음부터 구현할 때와 마찬가지로\n",
    "추정된 매개변수가 실제 매개변수와 거의 일치한다는 점에 유의하세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "210df6b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:07:16.634084Z",
     "iopub.status.busy": "2023-08-18T07:07:16.632077Z",
     "iopub.status.idle": "2023-08-18T07:07:16.640710Z",
     "shell.execute_reply": "2023-08-18T07:07:16.639658Z"
    },
    "origin_pos": 29,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "@d2l.add_to_class(LinearRegression)  #@save\n",
    "def get_w_b(self):\n",
    "    return (self.net.weight.data, self.net.bias.data)\n",
    "w, b = model.get_w_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5d3a52a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:07:16.647984Z",
     "iopub.status.busy": "2023-08-18T07:07:16.646094Z",
     "iopub.status.idle": "2023-08-18T07:07:16.658649Z",
     "shell.execute_reply": "2023-08-18T07:07:16.657503Z"
    },
    "origin_pos": 31,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in estimating w: tensor([ 0.0084, -0.0149])\n",
      "error in estimating b: tensor([0.0099])\n"
     ]
    }
   ],
   "source": [
    "print(f'error in estimating w: {data.w - w.reshape(data.w.shape)}')\n",
    "print(f'error in estimating b: {data.b - b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a56ac0",
   "metadata": {
    "origin_pos": 32
   },
   "source": [
    "## Summary\n",
    "\n",
    "This section contains the first\n",
    "implementation of a deep network (in this book)\n",
    "to tap into the conveniences afforded\n",
    "by modern deep learning frameworks,\n",
    "such as MXNet :cite:`Chen.Li.Li.ea.2015`, \n",
    "JAX :cite:`Frostig.Johnson.Leary.2018`, \n",
    "PyTorch :cite:`Paszke.Gross.Massa.ea.2019`, \n",
    "and Tensorflow :cite:`Abadi.Barham.Chen.ea.2016`.\n",
    "We used framework defaults for loading data, defining a layer,\n",
    "a loss function, an optimizer and a training loop.\n",
    "Whenever the framework provides all necessary features,\n",
    "it is generally a good idea to use them,\n",
    "since the library implementations of these components\n",
    "tend to be heavily optimized for performance\n",
    "and properly tested for reliability.\n",
    "At the same time, try not to forget\n",
    "that these modules *can* be implemented directly.\n",
    "This is especially important for aspiring researchers\n",
    "who wish to live on the leading edge of model development,\n",
    "where you will be inventing new components\n",
    "that cannot possibly exist in any current library.\n",
    "\n",
    "## 요약\n",
    "\n",
    "이 섹션에서는 MXNet(Chen.Li.Li.ea.2015), JAX(Frostig.Johnson.Leary.2018), PyTorch(Paszke.Gross.Massa.ea.2019), Tensorflow(Abadi.Barham.Chen.ea.2016)와 같은 최신 딥 러닝 프레임워크의 편의성을 활용하기 위한 딥 네트워크(이 책에서 처음 구현)를 소개합니다.데이터 로딩, 계층 정의, 손실 함수, 옵티마이저 및 학습 루프를 위해 프레임워크 기본값을 사용했습니다.프레임워크가 필요한 모든 기능을 제공하는 경우,\n",
    "일반적으로 이러한 구성 요소의 라이브러리 구현은 성능 최적화를 위해 고도로 최적화되고 신뢰성을 위해 적절하게 테스트되는 경향이 있으므로 프레임워크 기본값을 사용하는 것이 좋습니다.동시에, 이러한 모듈은 직접 구현할 수 있다는 점을 잊지 마세요.특히, 모델 개발의 선두에서 살고 싶어 하는 야심 찬 연구자들에게는 더욱 중요합니다.\n",
    "현재 어떤 라이브러리에도 존재할 수 없는 새로운 구성 요소를 개발하게 될 것입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159b0831",
   "metadata": {
    "origin_pos": 34,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "In PyTorch, the `data` module provides tools for data processing,\n",
    "the `nn` module defines a large number of neural network layers and common loss functions.\n",
    "We can initialize the parameters by replacing their values\n",
    "with methods ending with `_`.\n",
    "Note that we need to specify the input dimensions of the network.\n",
    "While this is trivial for now, it can have significant knock-on effects\n",
    "when we want to design complex networks with many layers.\n",
    "Careful considerations of how to parametrize these networks\n",
    "is needed to allow portability.\n",
    "\n",
    "PyTorch에서 `data` 모듈은 데이터 처리 도구를 제공하고,\n",
    "`nn` 모듈은 다수의 신경망 계층과 일반적인 손실 함수를 정의합니다.`_`로 끝나는 메서드로 값을 대체하여 매개변수를 초기화할 수 있습니다.네트워크의 입력 차원을 지정해야 합니다.지금은 간단하지만, 여러 계층을 가진 복잡한 네트워크를 설계할 때는 상당한 영향을 미칠 수 있습니다.이식성을 확보하려면 이러한 네트워크를 매개변수화하는 방법에 대한 신중한 고려가 필요합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1424ce10",
   "metadata": {
    "origin_pos": 36
   },
   "source": [
    "## Exercises\n",
    "## 연습\n",
    "\n",
    "1. How would you need to change the learning rate if you replace the aggregate loss over the minibatch\n",
    "   with an average over the loss on the minibatch?\n",
    "1. 미니배치의 총 손실을 미니배치 손실의 평균으로 대체하는 경우 학습률을 어떻게 변경해야 합니까?\n",
    "미니배치에 대한 손실을 총합(sum)에서 평균(average)으로 변경하면, **학습률을 비례적으로 늘려야 할 필요가 있을 수 있습니다.** 📈\n",
    "\n",
    "---\n",
    "\n",
    "## 손실 함수 변경의 영향\n",
    "\n",
    "### 1. 손실 총합 vs. 손실 평균\n",
    "* **총합 손실 (Sum Loss)**: 미니배치 내의 모든 샘플에 대한 손실 값을 단순히 합산합니다. 배치 크기가 $B$이고 각 샘플 $i$에 대한 손실이 $L_i$라면, 총합 손실은 $\\sum_{i=1}^{B} L_i$가 됩니다.\n",
    "* **평균 손실 (Average Loss)**: 미니배치 내의 모든 샘플에 대한 손실 값을 합산한 후 배치 크기 $B$로 나눕니다. 평균 손실은 $\\frac{1}{B} \\sum_{i=1}^{B} L_i$가 됩니다.\n",
    "\n",
    "### 2. 기울기(Gradient)에 미치는 영향\n",
    "최적화 알고리즘은 손실 함수의 기울기를 계산하여 매개변수를 업데이트합니다.\n",
    "* **총합 손실의 기울기**: 손실 총합에 대한 기울기는 각 샘플의 기울기를 합산한 것과 같습니다. 즉, $\\nabla (\\sum L_i) = \\sum \\nabla L_i$ 입니다.\n",
    "* **평균 손실의 기울기**: 손실 평균에 대한 기울기는 각 샘플의 기울기를 합산한 후 배치 크기 $B$로 나눈 것과 같습니다. 즉, $\\nabla (\\frac{1}{B} \\sum L_i) = \\frac{1}{B} \\sum \\nabla L_i$ 입니다.\n",
    "\n",
    "여기서 중요한 점은 **평균 손실을 사용하면 계산된 기울기의 크기가 총합 손실을 사용할 때보다 배치 크기 $B$만큼 작아진다는 것**입니다.\n",
    "\n",
    "### 3. 학습률 변경의 필요성\n",
    "매개변수 업데이트 규칙은 일반적으로 다음과 같습니다:\n",
    "$\\theta \\leftarrow \\theta - \\text{학습률} \\times \\text{기울기}$\n",
    "\n",
    "* **총합 손실 사용 시**: $\\theta \\leftarrow \\theta - \\text{학습률}_{\\text{sum}} \\times (\\sum \\nabla L_i)$\n",
    "* **평균 손실 사용 시**: $\\theta \\leftarrow \\theta - \\text{학습률}_{\\text{avg}} \\times (\\frac{1}{B} \\sum \\nabla L_i)$\n",
    "\n",
    "만약 우리가 동일한 양의 매개변수 업데이트를 원한다면, 즉 **기울기 업데이트의 '크기'를 동일하게 유지하고 싶다면**, 평균 손실을 사용할 때 학습률을 더 크게 설정해야 합니다.\n",
    "\n",
    "$\\text{학습률}_{\\text{sum}} \\times (\\sum \\nabla L_i) = \\text{학습률}_{\\text{avg}} \\times (\\frac{1}{B} \\sum \\nabla L_i)$\n",
    "\n",
    "위 식에서 $(\\sum \\nabla L_i)$ 항을 소거하면:\n",
    "\n",
    "$\\text{학습률}_{\\text{sum}} = \\text{학습률}_{\\text{avg}} / B$\n",
    "또는\n",
    "$\\text{학습률}_{\\text{avg}} = \\text{학습률}_{\\text{sum}} \\times B$\n",
    "\n",
    "따라서, 미니배치 손실을 총합에서 평균으로 변경할 경우, **평균 손실에 대한 학습률($\\text{학습률}_{\\text{avg}}$)은 총합 손실에 대한 학습률($\\text{학습률}_{\\text{sum}}$)에 배치 크기 $B$를 곱한 값으로 설정**해야 합니다. 즉, **학습률을 배치 크기만큼 늘려야 합니다.**\n",
    "\n",
    "---\n",
    "\n",
    "## 실제 적용과 고려사항\n",
    "\n",
    "실제로 대부분의 딥러닝 프레임워크와 구현에서는 손실 함수를 계산할 때 **기본적으로 미니배치에 대한 평균 손실을 사용**합니다. 이는 배치 크기가 바뀌더라도 학습률을 크게 조정할 필요 없이 일관된 학습 거동을 유지할 수 있기 때문입니다. 만약 당신이 직접 손실 함수를 구현하여 총합 손실을 반환하도록 했다가 평균 손실로 바꾸는 경우라면, 위에서 설명한 대로 학습률을 조정해야 합니다.\n",
    "\n",
    "**요약하자면,** 미니배치 손실을 총합에서 평균으로 바꾸면 기울기 크기가 배치 크기만큼 작아지므로, **동일한 학습 효과를 유지하기 위해서는 학습률을 배치 크기만큼 늘려야 합니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7a70ea",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. Review the framework documentation to see which loss functions are provided. In particular,\n",
    "   replace the squared loss with Huber's robust loss function. That is, use the loss function\n",
    "   $$l(y,y') = \\begin{cases}|y-y'| -\\frac{\\sigma}{2} & \\textrm{ if } |y-y'| > \\sigma \\\\ \\frac{1}{2 \\sigma} (y-y')^2 & \\textrm{ otherwise}\\end{cases}$$\n",
    "1. 프레임워크 설명서를 검토하여 어떤 손실 함수가 제공되는지 확인하세요.특히 제곱 손실을 Huber의 강건 손실 함수로 대체하세요.즉, 다음 손실 함수를 사용하세요.\n",
    "$$l(y,y') = \\begin{cases}|y-y'|-\\frac{\\sigma}{2} & \\textrm{ if } |y-y'|> \\sigma \\\\ \\frac{1}{2 \\sigma} (y-y')^2 & \\textrm{ otherwise}\\end{cases}$$\n",
    "\n",
    "각주에 명시된 후버 손실 함수의 수식은 일반적인 정의와 다소 차이가 있습니다. 제시된 수식은 아래와 같으므로, 이에 따라 후버 손실 함수를 구현하고 설명을 진행하겠습니다.\n",
    "\n",
    "$$\n",
    "l(y,y') =\n",
    "\\begin{cases}\n",
    "    |y - y'| - \\frac{1}{2}\\sigma & \\text{if } |y - y'| > \\sigma \\\\\n",
    "    \\frac{1}{2\\sigma}(y - y')^2 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "일반적으로 딥러닝 프레임워크는 다양한 손실 함수를 제공하며, `torch.nn.MSELoss` (평균 제곱 오차), `torch.nn.L1Loss` (평균 절대 오차) 등이 대표적입니다. 후버 손실(Huber Loss)은 PyTorch의 `torch.nn.HuberLoss`와 같은 형태로 직접 제공되기도 합니다.\n",
    "\n",
    "---\n",
    "\n",
    "## Huber 손실 함수 구현 및 설명 ⚖️\n",
    "\n",
    "주어진 수식에 따라 후버 손실 함수를 구현해 보겠습니다. 이 손실 함수는 오차의 크기에 따라 제곱 항과 절대값 항을 다르게 적용하여, 이상치에 덜 민감하면서도 최적화가 용이하도록 설계되었습니다.\n",
    "\n",
    "### 1. Huber 손실 함수 구현\n",
    "\n",
    "제시된 수식 $l(y,y') = \\frac{1}{2\\sigma}(y-y')^2$ (if $|y-y'| \\le \\sigma$)와 $l(y,y') = |y-y'| - \\frac{1}{2}\\sigma$ (if $|y-y'| > \\sigma$)에 따라 PyTorch로 구현합니다.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def huber_loss_custom(y_hat, y, sigma=1.0):\n",
    "    \"\"\"\n",
    "    주어진 수식에 따른 Huber 손실 함수 구현.\n",
    "    y_hat: 예측값\n",
    "    y: 실제값\n",
    "    sigma: 임계값 (델타 값)\n",
    "    \"\"\"\n",
    "    diff = torch.abs(y_hat - y)\n",
    "    \n",
    "    # |y - y'| <= sigma 인 경우\n",
    "    # 0.5 * (diff**2) / sigma\n",
    "    loss_small = 0.5 * (diff**2) / sigma \n",
    "    \n",
    "    # |y - y'| > sigma 인 경우\n",
    "    # diff - 0.5 * sigma\n",
    "    loss_large = diff - 0.5 * sigma\n",
    "    \n",
    "    # torch.where를 사용하여 조건에 따라 손실 값 선택\n",
    "    loss = torch.where(diff <= sigma, loss_small, loss_large)\n",
    "    \n",
    "    return loss.mean() # 일반적으로 배치 평균을 반환\n",
    "\n",
    "# 예시 사용\n",
    "y_hat = torch.tensor([1.0, 2.0, 3.0, 10.0, 20.0])\n",
    "y = torch.tensor([1.1, 2.2, 2.9, 0.0, 100.0]) # 0.0과 100.0은 이상치\n",
    "sigma_val = 1.0\n",
    "\n",
    "custom_huber = huber_loss_custom(y_hat, y, sigma_val)\n",
    "print(f\"커스텀 Huber 손실: {custom_huber.item()}\")\n",
    "\n",
    "# PyTorch 내장 HuberLoss (기본 delta=1.0)와 비교\n",
    "# PyTorch의 HuberLoss 공식은 0.5 * ((y-y')**2) 또는 delta * (|y-y'| - 0.5 * delta)\n",
    "# 이는 주어진 문제의 수식과 계수에서 약간 차이가 있음.\n",
    "# 따라서 직접 구현한 것과 내장 함수는 결과값이 다르게 나옴.\n",
    "# torch_huber_loss_fn = nn.HuberLoss(delta=sigma_val)\n",
    "# torch_huber = torch_huber_loss_fn(y_hat, y)\n",
    "# print(f\"PyTorch 내장 Huber 손실: {torch_huber.item()}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 손실 함수의 특징 분석\n",
    "\n",
    "제시된 Huber 손실 함수는 다음과 같은 특징을 가집니다:\n",
    "\n",
    "* **$\\sigma$ 값의 중요성**: `sigma`($\\sigma$) 값은 모델이 오차를 어떻게 처리할지에 대한 임계점을 정의합니다.\n",
    "    * **$|y - y'| \\le \\sigma$ (작은 오차)**: 오차가 `sigma`보다 작거나 같을 때는 $\\frac{1}{2\\sigma}(y - y')^2$ 형태의 **제곱 오차**를 사용합니다. 이는 오차가 작을 때 모델이 빠르게 수렴하도록 돕고, 미분이 가능하므로 최적화에 유리합니다. $\\sigma$로 나눠주기 때문에, $\\sigma$가 크면 손실 값이 작아지고, $\\sigma$가 작으면 손실 값이 커집니다.\n",
    "    * **$|y - y'| > \\sigma$ (큰 오차)**: 오차가 `sigma`보다 클 때는 $|y - y'| - \\frac{1}{2}\\sigma$ 형태의 **절대값 오차**를 사용합니다. 이는 이상치에 대해 손실 값이 급격히 증가하는 것을 방지하여 모델이 이상치에 덜 민감하게(더 **견고하게**) 만듭니다. $- \\frac{1}{2}\\sigma$ 항은 $a^2$에서 $a$로 전환될 때의 연속성을 보장하기 위함입니다.\n",
    "\n",
    "* **기울기 거동**:\n",
    "    * **작은 오차**: 오차가 작은 영역에서는 기울기가 선형적으로 감소합니다 ($\\frac{1}{\\sigma}(y - y')$). 이는 MSE와 유사하여 최적점 근처에서 부드럽게 수렴할 수 있도록 돕습니다.\n",
    "    * **큰 오차**: 오차가 큰 영역에서는 기울기가 $\\pm 1$로 일정합니다. 이는 L1 손실과 유사하게, 이상치가 있더라도 기울기가 극단적으로 커지지 않아 **경사 폭발(gradient explosion)**을 방지하고 학습 안정성을 높입니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 제곱 손실(MSE)과 비교\n",
    "\n",
    "* **이상치에 대한 견고성**:\n",
    "    * **MSE**: 오차를 제곱하므로 이상치에 대한 페널티가 매우 커집니다. 하나의 큰 이상치가 전체 손실 값을 지배하여 모델이 이 이상치를 줄이는 데 과도하게 집중하게 만듭니다.\n",
    "    * **Huber Loss**: 큰 오차에 대해서는 선형적인 페널티를 주므로, MSE보다 이상치에 훨씬 **견고합니다**. 모델이 이상치에 덜 휘둘리고 전체적인 데이터 분포에 더 잘 맞는 학습을 하도록 돕습니다.\n",
    "\n",
    "* **최적화 용이성**:\n",
    "    * **MSE**: 전체 구간에서 미분 가능하며 부드럽기 때문에 경사 하강법 기반의 최적화가 용이하고, 최적점으로 빠르게 수렴하는 경향이 있습니다.\n",
    "    * **Huber Loss**: $0$에서 미분 불가능한 L1 손실과 달리, Huber 손실은 $\\pm \\sigma$ 지점에서 연속적이고 미분 가능하도록 설계되어 있습니다 (물론 $y=y'$ 즉 오차가 0일 때는 기울기가 0이 됩니다). 이는 L1 손실보다 최적화가 더 안정적입니다.\n",
    "\n",
    "**결론적으로,** 제시된 형태의 후버 손실 함수는 제곱 손실의 부드러운 특성과 절대값 손실의 이상치에 대한 견고성을 결합하여, 실제 데이터에서 종종 발생하는 이상치 문제에 효과적으로 대응할 수 있는 강력한 대안이 됩니다. $\\sigma$ 값을 조절함으로써, 이상치에 대한 민감도를 조절할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd80d86",
   "metadata": {},
   "source": [
    "\n",
    "1. How do you access the gradient of the weights of the model?\n",
    "1. 모델 가중치의 기울기에 어떻게 접근합니까?\n",
    "\n",
    "모델 가중치의 기울기(gradient)는 딥러닝 프레임워크에서 매개변수와 함께 저장되며, 역전파(backpropagation)를 수행한 후 접근할 수 있습니다.\n",
    "\n",
    "-----\n",
    "\n",
    "## 모델 가중치의 기울기 접근 방법\n",
    "\n",
    "대부분의 딥러닝 프레임워크(예: PyTorch, TensorFlow)는 모델의 매개변수에 대한 기울기를 자동으로 계산하고 저장합니다. 이 기울기는 매개변수 업데이트에 사용됩니다.\n",
    "\n",
    "### 1\\. 역전파(Backpropagation) 수행\n",
    "\n",
    "가장 먼저 해야 할 일은 손실 함수(`loss`)를 정의하고, 이 손실에 대해 `backward()` 메서드를 호출하여 역전파를 수행하는 것입니다. 이 과정에서 모델의 모든 학습 가능한 매개변수에 대한 기울기가 계산됩니다.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 간단한 선형 모델 정의\n",
    "model = nn.Linear(10, 1) # 입력 10, 출력 1\n",
    "\n",
    "# 더미 입력 및 실제 값 생성\n",
    "X = torch.randn(5, 10) # 5개의 샘플, 각 샘플은 10개의 특성\n",
    "y = torch.randn(5, 1)\n",
    "\n",
    "# 예측 수행\n",
    "y_hat = model(X)\n",
    "\n",
    "# 손실 함수 정의 (예: MSE)\n",
    "loss_fn = nn.MSELoss()\n",
    "loss = loss_fn(y_hat, y)\n",
    "\n",
    "# 역전파 수행: 이 시점에서 가중치에 대한 기울기가 계산되어 저장됩니다.\n",
    "loss.backward()\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "### 2\\. 기울기(Gradient)에 접근\n",
    "\n",
    "역전파가 완료되면, 모델의 각 학습 가능한 매개변수(`Parameter` 객체)는 `grad` 속성에 해당 기울기 값을 저장하게 됩니다.\n",
    "\n",
    "```python\n",
    "# 모델의 가중치(weights)에 접근하여 기울기를 확인합니다.\n",
    "# nn.Linear 레이어는 'weight'와 'bias' 두 가지 매개변수를 가집니다.\n",
    "\n",
    "print(f\"가중치 텐서: {model.weight}\")\n",
    "print(f\"가중치의 기울기: {model.weight.grad}\") # <-- 여기에 기울기가 저장됩니다.\n",
    "\n",
    "print(f\"\\n편향 텐서: {model.bias}\")\n",
    "print(f\"편향의 기울기: {model.bias.grad}\") # <-- 편향의 기울기도 여기에 저장됩니다.\n",
    "```\n",
    "\n",
    "**설명:**\n",
    "\n",
    "  * `model.weight`: `nn.Linear` 레이어의 가중치 텐서에 접근합니다.\n",
    "  * `model.weight.grad`: 이 가중치 텐서에 대한 **기울기 텐서**에 접근합니다. `loss.backward()`가 호출된 후에만 이 속성에 유의미한 값이 할당됩니다.\n",
    "\n",
    "### 3\\. 모든 매개변수의 기울기 순회\n",
    "\n",
    "모델에 여러 레이어가 있거나 복잡한 구조를 가진 경우, `model.parameters()` 메서드를 사용하여 모든 학습 가능한 매개변수를 순회하며 기울기에 접근할 수 있습니다.\n",
    "\n",
    "```python\n",
    "print(\"\\n모든 모델 매개변수와 그 기울기:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad: # 학습 가능한 매개변수인지 확인\n",
    "        print(f\"매개변수 이름: {name}\")\n",
    "        print(f\"매개변수 값: {param.data}\") # 매개변수 자체의 값\n",
    "        print(f\"매개변수 기울기: {param.grad}\") # 매개변수의 기울기\n",
    "        print(\"-\" * 20)\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "## 중요 사항\n",
    "\n",
    "  * **기울기 누적**: 딥러닝 프레임워크는 기본적으로 기울기를 누적(accumulate)합니다. 따라서 여러 번 `loss.backward()`를 호출하기 전에 **항상 `optimizer.zero_grad()` 또는 `model.zero_grad()`를 호출하여 이전 기울기를 0으로 초기화**해야 합니다. 그렇지 않으면 기울기들이 합산되어 잘못된 업데이트가 일어날 수 있습니다.\n",
    "  * **옵티마이저의 역할**: 실제 모델 훈련에서는 계산된 기울기를 사용하여 `optimizer.step()`을 호출함으로써 모델의 매개변수를 업데이트합니다. 이 과정은 프레임워크 내부적으로 `param = param - learning_rate * param.grad`와 유사한 방식으로 이루어집니다.\n",
    "  * **`.data` vs. `.detach()`**: 기울기 값을 **수정하지 않고** 단순히 확인만 할 때는 `param.grad`를 직접 사용해도 됩니다. 그러나 만약 기울기 값을 변경하거나, 기울기 계산 그래프에 포함되지 않는 텐서를 얻고 싶다면 `.detach()` 메서드를 사용하는 것이 좋습니다. (`param.grad.data`는 레거시 방식이며, `.detach()`를 사용하는 것이 권장됩니다.)\n",
    "\n",
    "가중치의 기울기는 신경망이 학습 방향을 결정하는 데 사용하는 핵심 정보이며, 최적화 알고리즘은 이 기울기를 사용하여 모델의 성능을 점진적으로 개선합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e23d86",
   "metadata": {},
   "source": [
    "\n",
    "1. What is the effect on the solution if you change the learning rate and the number of epochs? Does it keep on improving?\n",
    "1. 학습률과 에포크 수를 변경하면 해에 어떤 영향이 있습니까?계속 향상됩니까?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4325556",
   "metadata": {},
   "source": [
    "\n",
    "1. How does the solution change as you vary the amount of data generated?\n",
    "    1. Plot the estimation error for $\\hat{\\mathbf{w}} - \\mathbf{w}$ and $\\hat{b} - b$ as a function of the amount of data. Hint: increase the amount of data logarithmically rather than linearly, i.e., 5, 10, 20, 50, ..., 10,000 rather than 1000, 2000, ..., 10,000.\n",
    "    2. Why is the suggestion in the hint appropriate?\n",
    "\n",
    "1. 생성되는 데이터 양을 변경하면 해는 어떻게 변합니까?\n",
    "    1. $\\hat{\\mathbf{w}} - \\mathbf{w}$와 $\\hat{b} - b$에 대한 추정 오차를 데이터 양의 함수로 나타내세요.힌트: 데이터 양을 선형적으로 증가시키지 않고 대수적으로 증가시키세요. 즉, 1000, 2000, ..., 10,000 대신 5, 10, 20, 50, ..., 10,000으로 증가시키세요.\n",
    "    2. 힌트의 제안이 적절한 이유는 무엇인가요?\n",
    "\n",
    "데이터 양을 변경함에 따라 해결책은 **추정 오차의 감소 방식**과 관련하여 변화합니다. 일반적으로 데이터 양이 증가할수록 모델 매개변수($\\hat{\\mathbf{w}}$ 및 $\\hat{b}$)의 추정 오차는 감소합니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 데이터 양에 따른 추정 오차 플로팅 📈\n",
    "\n",
    "데이터 양에 따른 추정 오차를 플로팅하려면 선형 회귀 모델을 가정하고, 다양한 데이터 포인트 수에 대해 모델을 훈련시킨 후 실제 값과 추정된 값 간의 오차를 계산해야 합니다.\n",
    "\n",
    "**개념 설명:**\n",
    "\n",
    "* **데이터 생성**: `n_examples`개의 샘플을 생성합니다. 각 샘플은 특성 $\\mathbf{x}$와 레이블 $y = \\mathbf{w}^\\top \\mathbf{x} + b + \\epsilon$으로 구성됩니다. 여기서 $\\epsilon$은 노이즈입니다.\n",
    "* **모델 훈련**: 생성된 데이터로 선형 회귀 모델을 훈련시킵니다.\n",
    "* **오차 계산**: 훈련된 모델로부터 얻은 추정된 가중치 $\\hat{\\mathbf{w}}$와 편향 $\\hat{b}$를 실제 가중치 $\\mathbf{w}$ 및 편향 $b$와 비교하여 오차($|\\hat{\\mathbf{w}} - \\mathbf{w}|$ 및 $|\\hat{b} - b|$)를 계산합니다.\n",
    "* **플로팅**: 이 오차 값을 데이터 양의 함수로 플로팅합니다.\n",
    "\n",
    "**일반적인 결과 예상:**\n",
    "\n",
    "데이터 양이 증가할수록 노이즈의 영향이 상쇄되고 데이터의 실제 분포가 더 잘 반영되므로, 모델은 실제 매개변수에 더 가깝게 학습됩니다. 따라서 $\\hat{\\mathbf{w}} - \\mathbf{w}$와 $\\hat{b} - b$의 **추정 오차는 점차 감소하는 경향**을 보일 것입니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 힌트의 제안이 적절한 이유 (로그 스케일 증가) 🤔\n",
    "\n",
    "힌트에서 데이터 양을 선형적으로(`1000, 2000, ..., 10000`) 늘리는 대신, 로그 스케일로(`5, 10, 20, 50, ..., 10000`) 늘리라고 제안한 것은 다음과 같은 이유 때문에 적절합니다.\n",
    "\n",
    "1.  **초기 변화의 민감한 포착**:\n",
    "    * 대부분의 통계적 추정 및 머신러닝 모델에서 **추정 오차는 데이터 양이 적을 때 가장 빠르게 감소**합니다. 즉, 데이터 양이 5개에서 50개로 증가할 때의 성능 향상은 10000개에서 10050개로 증가할 때의 성능 향상보다 훨씬 드라마틱합니다.\n",
    "    * 로그 스케일 증가는 작은 데이터 양 구간에 더 많은 측정 포인트를 할당하여, **초기 학습 곡선의 가파른 변화를 더 세밀하게 관찰**할 수 있게 해줍니다.\n",
    "\n",
    "2.  **수확 체감의 법칙 반영**:\n",
    "    * 데이터 양이 일정 수준 이상 증가하면, 추가적인 데이터가 모델 성능을 향상시키는 데 기여하는 정도가 점차 줄어듭니다. 이를 **수확 체감의 법칙(diminishing returns)**이라고 합니다.\n",
    "    * 선형 스케일로 데이터를 증가시키면, 대부분의 플롯이 데이터 양이 많아지는 후반부에 밀집되어 변화가 거의 없어 보이게 됩니다. 반면 로그 스케일은 이러한 수확 체감 구간을 **더 넓은 범위에 걸쳐 균등하게 시각화**하여, 성능 향상이 둔화되는 지점을 명확하게 보여줄 수 있습니다.\n",
    "\n",
    "3.  **로그-로그 또는 세미로그 플롯의 적합성**:\n",
    "    * 많은 경우, 추정 오차는 데이터 양의 함수로 **멱법칙(power-law) 관계**를 따르거나 **지수적 감소**를 보입니다. 이러한 관계는 로그 스케일 플롯(특히 로그-로그 플롯이나 세미로그 플롯)에서 직선이나 더 명확한 패턴으로 나타나기 때문에, 데이터의 본질적인 특성을 이해하는 데 도움이 됩니다.\n",
    "\n",
    "요약하자면, 로그 스케일로 데이터 양을 증가시키는 것은 **초기 단계의 급격한 개선과 후기 단계의 수확 체감 현상을 모두 효과적으로 시각화하고 분석**하는 데 매우 유용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566d9bde",
   "metadata": {
    "origin_pos": 38,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/45)\n",
    "\n",
    "[토론](https://discuss.d2l.ai/t/45)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
